{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boston housing priceing code same ha ussi py kam krhy hyn chapter 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sms \n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe=pd.read_csv('Concrete_Data_Yeh.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cement</th>\n",
       "      <th>slag</th>\n",
       "      <th>flyash</th>\n",
       "      <th>water</th>\n",
       "      <th>superplasticizer</th>\n",
       "      <th>coarseaggregate</th>\n",
       "      <th>fineaggregate</th>\n",
       "      <th>age</th>\n",
       "      <th>csMPa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>79.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>61.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270</td>\n",
       "      <td>40.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "      <td>41.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360</td>\n",
       "      <td>44.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cement   slag  flyash  water  superplasticizer  coarseaggregate  \\\n",
       "0   540.0    0.0     0.0  162.0               2.5           1040.0   \n",
       "1   540.0    0.0     0.0  162.0               2.5           1055.0   \n",
       "2   332.5  142.5     0.0  228.0               0.0            932.0   \n",
       "3   332.5  142.5     0.0  228.0               0.0            932.0   \n",
       "4   198.6  132.4     0.0  192.0               0.0            978.4   \n",
       "\n",
       "   fineaggregate  age  csMPa  \n",
       "0          676.0   28  79.99  \n",
       "1          676.0   28  61.89  \n",
       "2          594.0  270  40.27  \n",
       "3          594.0  365  41.05  \n",
       "4          825.5  360  44.30  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=dataframe.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cement</th>\n",
       "      <th>slag</th>\n",
       "      <th>flyash</th>\n",
       "      <th>water</th>\n",
       "      <th>superplasticizer</th>\n",
       "      <th>coarseaggregate</th>\n",
       "      <th>fineaggregate</th>\n",
       "      <th>age</th>\n",
       "      <th>csMPa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>985</td>\n",
       "      <td>255.3</td>\n",
       "      <td>98.8</td>\n",
       "      <td>77.0</td>\n",
       "      <td>188.6</td>\n",
       "      <td>6.5</td>\n",
       "      <td>919.0</td>\n",
       "      <td>749.3</td>\n",
       "      <td>28</td>\n",
       "      <td>33.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>281.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1104.0</td>\n",
       "      <td>774.0</td>\n",
       "      <td>28</td>\n",
       "      <td>22.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>816</td>\n",
       "      <td>525.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1125.0</td>\n",
       "      <td>613.0</td>\n",
       "      <td>7</td>\n",
       "      <td>42.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>513</td>\n",
       "      <td>424.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>8.9</td>\n",
       "      <td>822.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>7</td>\n",
       "      <td>40.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>168.0</td>\n",
       "      <td>42.1</td>\n",
       "      <td>163.8</td>\n",
       "      <td>121.8</td>\n",
       "      <td>5.7</td>\n",
       "      <td>1058.7</td>\n",
       "      <td>780.1</td>\n",
       "      <td>100</td>\n",
       "      <td>39.23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     cement  slag  flyash  water  superplasticizer  coarseaggregate  \\\n",
       "985   255.3  98.8    77.0  188.6               6.5            919.0   \n",
       "775   281.0   0.0     0.0  186.0               0.0           1104.0   \n",
       "816   525.0   0.0     0.0  189.0               0.0           1125.0   \n",
       "513   424.0  22.0   132.0  168.0               8.9            822.0   \n",
       "228   168.0  42.1   163.8  121.8               5.7           1058.7   \n",
       "\n",
       "     fineaggregate  age  csMPa  \n",
       "985          749.3   28  33.80  \n",
       "775          774.0   28  22.44  \n",
       "816          613.0    7  42.42  \n",
       "513          750.0    7  40.29  \n",
       "228          780.1  100  39.23  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete = df.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_data = concrete[:824,:8].astype('float') \n",
    "y_train_label = concrete[:824,8]\n",
    "x_test_data = concrete[824:,:8].astype('float')\n",
    "y_test_label = concrete[824:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(824, 8)\n",
      "(824,)\n",
      "(206, 8)\n",
      "(206,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_data.shape)\n",
    "print(y_train_label.shape)\n",
    "print(x_test_data.shape)\n",
    "print(y_test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=x_train_data.mean(axis=0)\n",
    "x_train_data-=mean\n",
    "std=x_train_data.std(axis=0)\n",
    "x_train_data/=std\n",
    "x_test_data-=mean\n",
    "x_test_data/=std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step: 3 slicing for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_val=x_train_data[:103] #totl 40k hain jisme se validation me 10k ja rahe hain\n",
    "partial_x_train=x_train_data[103:] #or baki partial me ja rahe ha\n",
    "y_val=y_train_label[:103]\n",
    "partial_y_train=y_train_label[103:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=models.Sequential()\n",
    "model.add(layers.Dense(64,activation='relu',input_shape=(partial_x_train.shape[1],)))\n",
    "model.add(layers.Dense(64,activation='relu'))\n",
    "model.add(layers.Dense(1))\n",
    "model.compile(optimizer='rmsprop',loss='mse',metrics=['mae'])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 721 samples, validate on 103 samples\n",
      "Epoch 1/100\n",
      "721/721 [==============================] - 0s 261us/step - loss: 1489.8627 - mae: 34.5517 - val_loss: 945.3545 - val_mae: 27.8548\n",
      "Epoch 2/100\n",
      "721/721 [==============================] - 0s 104us/step - loss: 1204.1323 - mae: 30.3662 - val_loss: 713.7203 - val_mae: 23.5521\n",
      "Epoch 3/100\n",
      "721/721 [==============================] - 0s 122us/step - loss: 887.7994 - mae: 25.2065 - val_loss: 466.7911 - val_mae: 18.2460\n",
      "Epoch 4/100\n",
      "721/721 [==============================] - 0s 105us/step - loss: 582.9743 - mae: 19.5425 - val_loss: 287.3588 - val_mae: 13.4759\n",
      "Epoch 5/100\n",
      "721/721 [==============================] - 0s 69us/step - loss: 357.1644 - mae: 15.0192 - val_loss: 175.3801 - val_mae: 9.7445\n",
      "Epoch 6/100\n",
      "721/721 [==============================] - 0s 60us/step - loss: 249.8480 - mae: 12.6974 - val_loss: 160.3305 - val_mae: 9.7925\n",
      "Epoch 7/100\n",
      "721/721 [==============================] - 0s 68us/step - loss: 222.0753 - mae: 11.9760 - val_loss: 157.6060 - val_mae: 9.8271\n",
      "Epoch 8/100\n",
      "721/721 [==============================] - 0s 68us/step - loss: 209.7424 - mae: 11.6611 - val_loss: 147.7017 - val_mae: 9.6102\n",
      "Epoch 9/100\n",
      "721/721 [==============================] - 0s 60us/step - loss: 198.7130 - mae: 11.4265 - val_loss: 140.0136 - val_mae: 9.3895\n",
      "Epoch 10/100\n",
      "721/721 [==============================] - 0s 78us/step - loss: 189.0425 - mae: 11.1808 - val_loss: 132.7394 - val_mae: 9.1905\n",
      "Epoch 11/100\n",
      "721/721 [==============================] - 0s 105us/step - loss: 181.1319 - mae: 10.9636 - val_loss: 129.8785 - val_mae: 9.2034\n",
      "Epoch 12/100\n",
      "721/721 [==============================] - 0s 104us/step - loss: 175.1398 - mae: 10.7871 - val_loss: 128.2522 - val_mae: 9.0742\n",
      "Epoch 13/100\n",
      "721/721 [==============================] - 0s 73us/step - loss: 168.9645 - mae: 10.6022 - val_loss: 123.3973 - val_mae: 8.9244\n",
      "Epoch 14/100\n",
      "721/721 [==============================] - 0s 90us/step - loss: 163.0561 - mae: 10.4224 - val_loss: 118.9891 - val_mae: 8.7380\n",
      "Epoch 15/100\n",
      "721/721 [==============================] - 0s 79us/step - loss: 158.5976 - mae: 10.2636 - val_loss: 115.2920 - val_mae: 8.6406\n",
      "Epoch 16/100\n",
      "721/721 [==============================] - 0s 64us/step - loss: 154.2733 - mae: 10.1239 - val_loss: 112.5837 - val_mae: 8.6119\n",
      "Epoch 17/100\n",
      "721/721 [==============================] - 0s 75us/step - loss: 150.6671 - mae: 9.9594 - val_loss: 110.9296 - val_mae: 8.5381\n",
      "Epoch 18/100\n",
      "721/721 [==============================] - 0s 82us/step - loss: 146.0596 - mae: 9.8268 - val_loss: 108.5781 - val_mae: 8.3875\n",
      "Epoch 19/100\n",
      "721/721 [==============================] - 0s 96us/step - loss: 143.6099 - mae: 9.7505 - val_loss: 107.0695 - val_mae: 8.4072\n",
      "Epoch 20/100\n",
      "721/721 [==============================] - 0s 123us/step - loss: 140.8984 - mae: 9.6428 - val_loss: 103.5493 - val_mae: 8.2036\n",
      "Epoch 21/100\n",
      "721/721 [==============================] - 0s 115us/step - loss: 139.1000 - mae: 9.6025 - val_loss: 101.4589 - val_mae: 8.0964\n",
      "Epoch 22/100\n",
      "721/721 [==============================] - 0s 130us/step - loss: 136.8532 - mae: 9.5305 - val_loss: 101.9562 - val_mae: 8.0923\n",
      "Epoch 23/100\n",
      "721/721 [==============================] - 0s 68us/step - loss: 134.0450 - mae: 9.3938 - val_loss: 102.2280 - val_mae: 8.1246\n",
      "Epoch 24/100\n",
      "721/721 [==============================] - 0s 57us/step - loss: 132.2805 - mae: 9.3025 - val_loss: 99.4959 - val_mae: 8.0200\n",
      "Epoch 25/100\n",
      "721/721 [==============================] - 0s 79us/step - loss: 130.9753 - mae: 9.2963 - val_loss: 98.1252 - val_mae: 7.9900\n",
      "Epoch 26/100\n",
      "721/721 [==============================] - 0s 57us/step - loss: 128.8171 - mae: 9.2158 - val_loss: 98.9840 - val_mae: 8.0387\n",
      "Epoch 27/100\n",
      "721/721 [==============================] - 0s 75us/step - loss: 126.7203 - mae: 9.1168 - val_loss: 97.0427 - val_mae: 7.9687\n",
      "Epoch 28/100\n",
      "721/721 [==============================] - 0s 80us/step - loss: 125.2949 - mae: 9.0461 - val_loss: 97.5109 - val_mae: 7.9653\n",
      "Epoch 29/100\n",
      "721/721 [==============================] - 0s 134us/step - loss: 123.4532 - mae: 8.9798 - val_loss: 98.1344 - val_mae: 8.0068\n",
      "Epoch 30/100\n",
      "721/721 [==============================] - 0s 104us/step - loss: 121.6488 - mae: 8.9038 - val_loss: 94.5156 - val_mae: 7.8225\n",
      "Epoch 31/100\n",
      "721/721 [==============================] - 0s 94us/step - loss: 119.7456 - mae: 8.8165 - val_loss: 92.0313 - val_mae: 7.7885\n",
      "Epoch 32/100\n",
      "721/721 [==============================] - 0s 57us/step - loss: 117.3957 - mae: 8.7174 - val_loss: 89.0223 - val_mae: 7.6115\n",
      "Epoch 33/100\n",
      "721/721 [==============================] - 0s 71us/step - loss: 115.5273 - mae: 8.6589 - val_loss: 89.3779 - val_mae: 7.4852\n",
      "Epoch 34/100\n",
      "721/721 [==============================] - 0s 58us/step - loss: 113.7919 - mae: 8.6255 - val_loss: 87.0648 - val_mae: 7.4174\n",
      "Epoch 35/100\n",
      "721/721 [==============================] - 0s 79us/step - loss: 111.7668 - mae: 8.5524 - val_loss: 87.4844 - val_mae: 7.4242\n",
      "Epoch 36/100\n",
      "721/721 [==============================] - 0s 79us/step - loss: 110.3877 - mae: 8.4267 - val_loss: 84.9989 - val_mae: 7.4057\n",
      "Epoch 37/100\n",
      "721/721 [==============================] - 0s 130us/step - loss: 107.5033 - mae: 8.3185 - val_loss: 92.9056 - val_mae: 7.7749\n",
      "Epoch 38/100\n",
      "721/721 [==============================] - 0s 104us/step - loss: 106.3192 - mae: 8.2137 - val_loss: 82.5278 - val_mae: 7.2338\n",
      "Epoch 39/100\n",
      "721/721 [==============================] - 0s 80us/step - loss: 103.6544 - mae: 8.1698 - val_loss: 81.0486 - val_mae: 7.1503\n",
      "Epoch 40/100\n",
      "721/721 [==============================] - 0s 68us/step - loss: 101.4937 - mae: 8.0774 - val_loss: 84.4035 - val_mae: 7.4305\n",
      "Epoch 41/100\n",
      "721/721 [==============================] - 0s 104us/step - loss: 98.6893 - mae: 7.9162 - val_loss: 79.8120 - val_mae: 7.2310\n",
      "Epoch 42/100\n",
      "721/721 [==============================] - 0s 110us/step - loss: 97.0670 - mae: 7.8689 - val_loss: 83.8006 - val_mae: 7.3854\n",
      "Epoch 43/100\n",
      "721/721 [==============================] - 0s 93us/step - loss: 94.9993 - mae: 7.7666 - val_loss: 75.8646 - val_mae: 7.0126\n",
      "Epoch 44/100\n",
      "721/721 [==============================] - 0s 128us/step - loss: 92.7166 - mae: 7.7064 - val_loss: 74.4901 - val_mae: 6.8359\n",
      "Epoch 45/100\n",
      "721/721 [==============================] - 0s 86us/step - loss: 89.8606 - mae: 7.5729 - val_loss: 75.3037 - val_mae: 6.9477\n",
      "Epoch 46/100\n",
      "721/721 [==============================] - 0s 73us/step - loss: 87.7544 - mae: 7.4239 - val_loss: 71.7526 - val_mae: 6.7511\n",
      "Epoch 47/100\n",
      "721/721 [==============================] - 0s 83us/step - loss: 85.2064 - mae: 7.3302 - val_loss: 72.5054 - val_mae: 6.8692\n",
      "Epoch 48/100\n",
      "721/721 [==============================] - 0s 97us/step - loss: 83.4687 - mae: 7.2207 - val_loss: 68.6800 - val_mae: 6.6198\n",
      "Epoch 49/100\n",
      "721/721 [==============================] - 0s 80us/step - loss: 81.4312 - mae: 7.1542 - val_loss: 67.4517 - val_mae: 6.4935\n",
      "Epoch 50/100\n",
      "721/721 [==============================] - 0s 82us/step - loss: 78.9240 - mae: 7.0483 - val_loss: 66.6062 - val_mae: 6.3547\n",
      "Epoch 51/100\n",
      "721/721 [==============================] - 0s 69us/step - loss: 77.3430 - mae: 6.9438 - val_loss: 64.6664 - val_mae: 6.3284\n",
      "Epoch 52/100\n",
      "721/721 [==============================] - 0s 100us/step - loss: 74.5976 - mae: 6.8362 - val_loss: 63.2267 - val_mae: 6.2617\n",
      "Epoch 53/100\n",
      "721/721 [==============================] - 0s 103us/step - loss: 73.0518 - mae: 6.7157 - val_loss: 61.8839 - val_mae: 6.1920\n",
      "Epoch 54/100\n",
      "721/721 [==============================] - 0s 78us/step - loss: 70.9170 - mae: 6.6204 - val_loss: 61.1494 - val_mae: 6.1146\n",
      "Epoch 55/100\n",
      "721/721 [==============================] - 0s 61us/step - loss: 69.9299 - mae: 6.5413 - val_loss: 57.4774 - val_mae: 5.9387\n",
      "Epoch 56/100\n",
      "721/721 [==============================] - 0s 100us/step - loss: 67.9822 - mae: 6.4968 - val_loss: 59.5952 - val_mae: 6.0482\n",
      "Epoch 57/100\n",
      "721/721 [==============================] - 0s 67us/step - loss: 66.1099 - mae: 6.3659 - val_loss: 62.3182 - val_mae: 6.2549\n",
      "Epoch 58/100\n",
      "721/721 [==============================] - 0s 68us/step - loss: 65.1953 - mae: 6.3313 - val_loss: 56.1932 - val_mae: 5.9046\n",
      "Epoch 59/100\n",
      "721/721 [==============================] - 0s 57us/step - loss: 63.6873 - mae: 6.2187 - val_loss: 52.3291 - val_mae: 5.6522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100\n",
      "721/721 [==============================] - 0s 72us/step - loss: 61.9422 - mae: 6.1643 - val_loss: 54.3440 - val_mae: 5.8167\n",
      "Epoch 61/100\n",
      "721/721 [==============================] - 0s 62us/step - loss: 60.6643 - mae: 6.0710 - val_loss: 50.8735 - val_mae: 5.3852\n",
      "Epoch 62/100\n",
      "721/721 [==============================] - 0s 69us/step - loss: 59.7782 - mae: 5.9883 - val_loss: 48.7903 - val_mae: 5.4126\n",
      "Epoch 63/100\n",
      "721/721 [==============================] - 0s 71us/step - loss: 58.2253 - mae: 5.9098 - val_loss: 47.2546 - val_mae: 5.3152\n",
      "Epoch 64/100\n",
      "721/721 [==============================] - 0s 57us/step - loss: 57.4525 - mae: 5.8400 - val_loss: 45.7112 - val_mae: 5.1372\n",
      "Epoch 65/100\n",
      "721/721 [==============================] - 0s 53us/step - loss: 56.8364 - mae: 5.7922 - val_loss: 45.9259 - val_mae: 5.1760\n",
      "Epoch 66/100\n",
      "721/721 [==============================] - 0s 60us/step - loss: 55.4796 - mae: 5.7587 - val_loss: 45.6619 - val_mae: 5.2513\n",
      "Epoch 67/100\n",
      "721/721 [==============================] - 0s 57us/step - loss: 54.9059 - mae: 5.7257 - val_loss: 42.9209 - val_mae: 5.0689\n",
      "Epoch 68/100\n",
      "721/721 [==============================] - 0s 64us/step - loss: 53.0828 - mae: 5.5952 - val_loss: 44.5413 - val_mae: 4.9809\n",
      "Epoch 69/100\n",
      "721/721 [==============================] - 0s 75us/step - loss: 53.1490 - mae: 5.5906 - val_loss: 42.4515 - val_mae: 5.0289\n",
      "Epoch 70/100\n",
      "721/721 [==============================] - 0s 77us/step - loss: 52.5350 - mae: 5.5165 - val_loss: 42.9834 - val_mae: 5.0083\n",
      "Epoch 71/100\n",
      "721/721 [==============================] - 0s 61us/step - loss: 51.1011 - mae: 5.4790 - val_loss: 39.5402 - val_mae: 4.6829\n",
      "Epoch 72/100\n",
      "721/721 [==============================] - 0s 69us/step - loss: 50.2796 - mae: 5.4213 - val_loss: 38.2467 - val_mae: 4.6890\n",
      "Epoch 73/100\n",
      "721/721 [==============================] - 0s 64us/step - loss: 49.7205 - mae: 5.3874 - val_loss: 37.9166 - val_mae: 4.6825\n",
      "Epoch 74/100\n",
      "721/721 [==============================] - 0s 58us/step - loss: 49.3871 - mae: 5.3464 - val_loss: 37.8728 - val_mae: 4.6461\n",
      "Epoch 75/100\n",
      "721/721 [==============================] - 0s 51us/step - loss: 48.0712 - mae: 5.2767 - val_loss: 41.8297 - val_mae: 5.0264\n",
      "Epoch 76/100\n",
      "721/721 [==============================] - 0s 78us/step - loss: 48.7659 - mae: 5.3202 - val_loss: 37.2497 - val_mae: 4.6580\n",
      "Epoch 77/100\n",
      "721/721 [==============================] - 0s 60us/step - loss: 47.0965 - mae: 5.1974 - val_loss: 38.1254 - val_mae: 4.7353\n",
      "Epoch 78/100\n",
      "721/721 [==============================] - 0s 60us/step - loss: 46.1086 - mae: 5.1706 - val_loss: 38.1258 - val_mae: 4.6197\n",
      "Epoch 79/100\n",
      "721/721 [==============================] - 0s 79us/step - loss: 46.3969 - mae: 5.1550 - val_loss: 36.4654 - val_mae: 4.5841\n",
      "Epoch 80/100\n",
      "721/721 [==============================] - 0s 67us/step - loss: 45.3814 - mae: 5.0900 - val_loss: 35.5777 - val_mae: 4.5120\n",
      "Epoch 81/100\n",
      "721/721 [==============================] - 0s 93us/step - loss: 44.6271 - mae: 5.0419 - val_loss: 34.6721 - val_mae: 4.3689\n",
      "Epoch 82/100\n",
      "721/721 [==============================] - 0s 112us/step - loss: 44.5914 - mae: 5.0400 - val_loss: 33.6111 - val_mae: 4.3374\n",
      "Epoch 83/100\n",
      "721/721 [==============================] - 0s 112us/step - loss: 43.6226 - mae: 5.0227 - val_loss: 32.6545 - val_mae: 4.3397\n",
      "Epoch 84/100\n",
      "721/721 [==============================] - 0s 104us/step - loss: 44.1613 - mae: 5.0278 - val_loss: 32.1513 - val_mae: 4.2970\n",
      "Epoch 85/100\n",
      "721/721 [==============================] - 0s 112us/step - loss: 43.5523 - mae: 4.9718 - val_loss: 32.2104 - val_mae: 4.3017\n",
      "Epoch 86/100\n",
      "721/721 [==============================] - 0s 96us/step - loss: 42.6295 - mae: 4.9621 - val_loss: 35.3655 - val_mae: 4.4488\n",
      "Epoch 87/100\n",
      "721/721 [==============================] - 0s 72us/step - loss: 42.0144 - mae: 4.9025 - val_loss: 30.9775 - val_mae: 4.2241\n",
      "Epoch 88/100\n",
      "721/721 [==============================] - 0s 98us/step - loss: 41.6506 - mae: 4.8851 - val_loss: 31.4939 - val_mae: 4.2251\n",
      "Epoch 89/100\n",
      "721/721 [==============================] - 0s 101us/step - loss: 40.8603 - mae: 4.8452 - val_loss: 33.4797 - val_mae: 4.4916\n",
      "Epoch 90/100\n",
      "721/721 [==============================] - 0s 85us/step - loss: 40.4970 - mae: 4.7955 - val_loss: 34.2434 - val_mae: 4.5539\n",
      "Epoch 91/100\n",
      "721/721 [==============================] - 0s 121us/step - loss: 40.2511 - mae: 4.7766 - val_loss: 30.6784 - val_mae: 4.1993\n",
      "Epoch 92/100\n",
      "721/721 [==============================] - 0s 110us/step - loss: 40.1157 - mae: 4.7713 - val_loss: 30.5183 - val_mae: 4.1591\n",
      "Epoch 93/100\n",
      "721/721 [==============================] - 0s 83us/step - loss: 39.6913 - mae: 4.7402 - val_loss: 29.3904 - val_mae: 4.1680\n",
      "Epoch 94/100\n",
      "721/721 [==============================] - 0s 64us/step - loss: 39.3908 - mae: 4.6885 - val_loss: 30.8404 - val_mae: 4.2961\n",
      "Epoch 95/100\n",
      "721/721 [==============================] - 0s 69us/step - loss: 38.5495 - mae: 4.6885 - val_loss: 30.0358 - val_mae: 4.1321\n",
      "Epoch 96/100\n",
      "721/721 [==============================] - 0s 71us/step - loss: 38.7944 - mae: 4.6989 - val_loss: 29.1785 - val_mae: 4.0798\n",
      "Epoch 97/100\n",
      "721/721 [==============================] - 0s 82us/step - loss: 37.7015 - mae: 4.6252 - val_loss: 33.0127 - val_mae: 4.2811\n",
      "Epoch 98/100\n",
      "721/721 [==============================] - 0s 122us/step - loss: 38.1919 - mae: 4.6581 - val_loss: 29.3277 - val_mae: 4.1711\n",
      "Epoch 99/100\n",
      "721/721 [==============================] - 0s 101us/step - loss: 37.4822 - mae: 4.6414 - val_loss: 30.8338 - val_mae: 4.2194\n",
      "Epoch 100/100\n",
      "721/721 [==============================] - 0s 83us/step - loss: 36.8936 - mae: 4.5992 - val_loss: 30.1305 - val_mae: 4.1833\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(partial_x_train,partial_y_train,epochs=100,batch_size=30,validation_data=(x_val,y_val)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.13053901450148\n",
      "4.183291435241699\n"
     ]
    }
   ],
   "source": [
    "val_mse,val_mae=model.evaluate(x_val,y_val,verbose=0)\n",
    "print(val_mse)\n",
    "print(val_mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict=history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-f73d53ea513f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0macc_values\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhistory_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mval_acc_values\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhistory_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'accuracy'"
     ]
    }
   ],
   "source": [
    "\n",
    "loss_values=history_dict['loss']\n",
    "val_loss_values=history_dict['val_loss']\n",
    "epochs=range(1,len(loss_values)+1)\n",
    "plt.plot(epochs,loss_values,'bo',label='Training loss')\n",
    "plt.plot(epochs,val_loss_values,'b',label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.clf\n",
    "acc_values=history_dict['accuracy']\n",
    "val_acc_values=history_dict['val_accuracy']\n",
    "epochs=range(1,len(acc_values)+1)\n",
    "plt.plot(epochs,acc_values,'bo',label='Training acc')\n",
    "plt.plot(epochs,val_acc_values,'b',label='Validation acc')\n",
    "plt.title('Training and Validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('acc')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def built_model():\n",
    "    model=models.Sequential() \n",
    "    #layers.Dense jb likhty hyn jb tensor flow frontend py chale\n",
    "    #backend py dense krty hyn bs \n",
    "    model.add(layers.Dense(8,activation='tanh',input_shape=(xtrain.shape[1],))) # 8 jo likha ha yh nodes hyn\n",
    "    model.add(layers.Dense(6,activation='tanh'))\n",
    "    model.add(layers.Dense(1))\n",
    "    #compile model using mse as a measure of model performance\n",
    "    model.compile(optimizer='rmsprop',loss='mse',metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 824 samples, validate on 206 samples\n",
      "Epoch 1/200\n",
      "824/824 [==============================] - 0s 190us/step - loss: 1567.8572 - mae: 35.9160 - val_loss: 1473.1259 - val_mae: 34.5712\n",
      "Epoch 2/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 1531.6155 - mae: 35.4026 - val_loss: 1440.3710 - val_mae: 34.0920\n",
      "Epoch 3/200\n",
      "824/824 [==============================] - 0s 76us/step - loss: 1497.9303 - mae: 34.9215 - val_loss: 1408.0583 - val_mae: 33.6126\n",
      "Epoch 4/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 1465.2384 - mae: 34.4445 - val_loss: 1376.7876 - val_mae: 33.1422\n",
      "Epoch 5/200\n",
      "824/824 [==============================] - 0s 76us/step - loss: 1433.5647 - mae: 33.9853 - val_loss: 1346.8294 - val_mae: 32.6856\n",
      "Epoch 6/200\n",
      "824/824 [==============================] - 0s 76us/step - loss: 1404.1530 - mae: 33.5449 - val_loss: 1320.0646 - val_mae: 32.2762\n",
      "Epoch 7/200\n",
      "824/824 [==============================] - 0s 76us/step - loss: 1377.0502 - mae: 33.1403 - val_loss: 1294.5318 - val_mae: 31.8830\n",
      "Epoch 8/200\n",
      "824/824 [==============================] - 0s 95us/step - loss: 1352.1473 - mae: 32.7612 - val_loss: 1272.0190 - val_mae: 31.5311\n",
      "Epoch 9/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 1329.9390 - mae: 32.4253 - val_loss: 1251.8139 - val_mae: 31.2120\n",
      "Epoch 10/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 1310.5564 - mae: 32.1207 - val_loss: 1234.3286 - val_mae: 30.9334\n",
      "Epoch 11/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 1293.3080 - mae: 31.8522 - val_loss: 1218.4142 - val_mae: 30.6776\n",
      "Epoch 12/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 1277.5900 - mae: 31.6056 - val_loss: 1204.0301 - val_mae: 30.4446\n",
      "Epoch 13/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 1263.2323 - mae: 31.3797 - val_loss: 1190.5876 - val_mae: 30.2252\n",
      "Epoch 14/200\n",
      "824/824 [==============================] - 0s 76us/step - loss: 1249.7614 - mae: 31.1658 - val_loss: 1178.0162 - val_mae: 30.0205\n",
      "Epoch 15/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 1237.0728 - mae: 30.9633 - val_loss: 1166.0815 - val_mae: 29.8249\n",
      "Epoch 16/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 1224.9467 - mae: 30.7687 - val_loss: 1154.4714 - val_mae: 29.6334\n",
      "Epoch 17/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 1213.1153 - mae: 30.5771 - val_loss: 1143.2701 - val_mae: 29.4474\n",
      "Epoch 18/200\n",
      "824/824 [==============================] - 0s 76us/step - loss: 1201.5437 - mae: 30.3910 - val_loss: 1132.1903 - val_mae: 29.2622\n",
      "Epoch 19/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 1190.2384 - mae: 30.2051 - val_loss: 1121.3826 - val_mae: 29.0803\n",
      "Epoch 20/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 1179.0940 - mae: 30.0222 - val_loss: 1110.7220 - val_mae: 28.8998\n",
      "Epoch 21/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 1168.1349 - mae: 29.8413 - val_loss: 1100.1911 - val_mae: 28.7203\n",
      "Epoch 22/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 1157.2274 - mae: 29.6609 - val_loss: 1089.6784 - val_mae: 28.5400\n",
      "Epoch 23/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 1146.4449 - mae: 29.4813 - val_loss: 1079.3524 - val_mae: 28.3618\n",
      "Epoch 24/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 1135.8420 - mae: 29.3027 - val_loss: 1069.1794 - val_mae: 28.1850\n",
      "Epoch 25/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 1125.2794 - mae: 29.1255 - val_loss: 1059.0110 - val_mae: 28.0072\n",
      "Epoch 26/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 1114.8044 - mae: 28.9494 - val_loss: 1048.9731 - val_mae: 27.8305\n",
      "Epoch 27/200\n",
      "824/824 [==============================] - 0s 76us/step - loss: 1104.2800 - mae: 28.7745 - val_loss: 1038.7903 - val_mae: 27.6501\n",
      "Epoch 28/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 1093.9404 - mae: 28.5976 - val_loss: 1028.9676 - val_mae: 27.4755\n",
      "Epoch 29/200\n",
      "824/824 [==============================] - 0s 76us/step - loss: 1083.7145 - mae: 28.4228 - val_loss: 1019.0727 - val_mae: 27.2996\n",
      "Epoch 30/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 1073.4870 - mae: 28.2479 - val_loss: 1009.2963 - val_mae: 27.1273\n",
      "Epoch 31/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 1063.3625 - mae: 28.0740 - val_loss: 999.6340 - val_mae: 26.9578\n",
      "Epoch 32/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 1053.2273 - mae: 27.9007 - val_loss: 989.8801 - val_mae: 26.7862\n",
      "Epoch 33/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 1043.1915 - mae: 27.7279 - val_loss: 980.2450 - val_mae: 26.6156\n",
      "Epoch 34/200\n",
      "824/824 [==============================] - 0s 76us/step - loss: 1033.3097 - mae: 27.5550 - val_loss: 970.8065 - val_mae: 26.4489\n",
      "Epoch 35/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 1023.3883 - mae: 27.3842 - val_loss: 961.2862 - val_mae: 26.2797\n",
      "Epoch 36/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 1013.5701 - mae: 27.2128 - val_loss: 951.8408 - val_mae: 26.1107\n",
      "Epoch 37/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 1003.7829 - mae: 27.0404 - val_loss: 942.5622 - val_mae: 25.9435\n",
      "Epoch 38/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 994.1990 - mae: 26.8694 - val_loss: 933.3566 - val_mae: 25.7764\n",
      "Epoch 39/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 984.5384 - mae: 26.6998 - val_loss: 924.0664 - val_mae: 25.6067\n",
      "Epoch 40/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 974.9550 - mae: 26.5296 - val_loss: 914.9536 - val_mae: 25.4395\n",
      "Epoch 41/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 965.5037 - mae: 26.3617 - val_loss: 905.9034 - val_mae: 25.2769\n",
      "Epoch 42/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 956.0477 - mae: 26.1924 - val_loss: 896.8570 - val_mae: 25.1133\n",
      "Epoch 43/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 946.6962 - mae: 26.0264 - val_loss: 887.8981 - val_mae: 24.9501\n",
      "Epoch 44/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 937.4000 - mae: 25.8600 - val_loss: 879.0383 - val_mae: 24.7875\n",
      "Epoch 45/200\n",
      "824/824 [==============================] - 0s 76us/step - loss: 928.2139 - mae: 25.6955 - val_loss: 870.2843 - val_mae: 24.6256\n",
      "Epoch 46/200\n",
      "824/824 [==============================] - 0s 114us/step - loss: 918.9880 - mae: 25.5321 - val_loss: 861.4412 - val_mae: 24.4624\n",
      "Epoch 47/200\n",
      "824/824 [==============================] - 0s 95us/step - loss: 909.8958 - mae: 25.3697 - val_loss: 852.8058 - val_mae: 24.3038\n",
      "Epoch 48/200\n",
      "824/824 [==============================] - 0s 95us/step - loss: 900.9050 - mae: 25.2063 - val_loss: 844.1911 - val_mae: 24.1445\n",
      "Epoch 49/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 891.9881 - mae: 25.0443 - val_loss: 835.7110 - val_mae: 23.9864\n",
      "Epoch 50/200\n",
      "824/824 [==============================] - 0s 76us/step - loss: 883.0201 - mae: 24.8837 - val_loss: 827.1348 - val_mae: 23.8268\n",
      "Epoch 51/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 874.1478 - mae: 24.7217 - val_loss: 818.7518 - val_mae: 23.6698\n",
      "Epoch 52/200\n",
      "824/824 [==============================] - 0s 66us/step - loss: 865.3695 - mae: 24.5622 - val_loss: 810.3224 - val_mae: 23.5108\n",
      "Epoch 53/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 856.6595 - mae: 24.4026 - val_loss: 802.1389 - val_mae: 23.3563\n",
      "Epoch 54/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 848.0049 - mae: 24.2450 - val_loss: 793.8473 - val_mae: 23.2000\n",
      "Epoch 55/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 839.5458 - mae: 24.0896 - val_loss: 785.7983 - val_mae: 23.0480\n",
      "Epoch 56/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 831.0517 - mae: 23.9337 - val_loss: 777.7414 - val_mae: 22.8969\n",
      "Epoch 57/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 822.6718 - mae: 23.7790 - val_loss: 769.7683 - val_mae: 22.7474\n",
      "Epoch 58/200\n",
      "824/824 [==============================] - 0s 76us/step - loss: 814.3264 - mae: 23.6264 - val_loss: 761.7781 - val_mae: 22.5963\n",
      "Epoch 59/200\n",
      "824/824 [==============================] - 0s 76us/step - loss: 805.9855 - mae: 23.4724 - val_loss: 753.8665 - val_mae: 22.4454\n",
      "Epoch 60/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 797.7716 - mae: 23.3218 - val_loss: 746.0756 - val_mae: 22.2963\n",
      "Epoch 61/200\n",
      "824/824 [==============================] - 0s 114us/step - loss: 789.5143 - mae: 23.1726 - val_loss: 738.2798 - val_mae: 22.1503\n",
      "Epoch 62/200\n",
      "824/824 [==============================] - 0s 95us/step - loss: 781.5193 - mae: 23.0258 - val_loss: 730.7308 - val_mae: 22.0091\n",
      "Epoch 63/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 773.4712 - mae: 22.8800 - val_loss: 723.0106 - val_mae: 21.8640\n",
      "Epoch 64/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 765.5210 - mae: 22.7358 - val_loss: 715.5001 - val_mae: 21.7216\n",
      "Epoch 65/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 757.5397 - mae: 22.5892 - val_loss: 707.9320 - val_mae: 21.5773\n",
      "Epoch 66/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 749.7521 - mae: 22.4455 - val_loss: 700.6093 - val_mae: 21.4378\n",
      "Epoch 67/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 742.0466 - mae: 22.3037 - val_loss: 693.2570 - val_mae: 21.2965\n",
      "Epoch 68/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 734.2639 - mae: 22.1632 - val_loss: 685.8747 - val_mae: 21.1547\n",
      "Epoch 69/200\n",
      "824/824 [==============================] - 0s 76us/step - loss: 726.6501 - mae: 22.0210 - val_loss: 678.7538 - val_mae: 21.0171\n",
      "Epoch 70/200\n",
      "824/824 [==============================] - 0s 114us/step - loss: 719.1286 - mae: 21.8840 - val_loss: 671.6738 - val_mae: 20.8797\n",
      "Epoch 71/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 711.6239 - mae: 21.7450 - val_loss: 664.4896 - val_mae: 20.7402\n",
      "Epoch 72/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 704.1579 - mae: 21.6098 - val_loss: 657.5421 - val_mae: 20.6069\n",
      "Epoch 73/200\n",
      "824/824 [==============================] - 0s 76us/step - loss: 696.7990 - mae: 21.4762 - val_loss: 650.4980 - val_mae: 20.4766\n",
      "Epoch 74/200\n",
      "824/824 [==============================] - 0s 76us/step - loss: 689.4623 - mae: 21.3407 - val_loss: 643.5964 - val_mae: 20.3513\n",
      "Epoch 75/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 682.1544 - mae: 21.2075 - val_loss: 636.7388 - val_mae: 20.2270\n",
      "Epoch 76/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 675.0010 - mae: 21.0762 - val_loss: 629.9982 - val_mae: 20.1037\n",
      "Epoch 77/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 667.9610 - mae: 20.9448 - val_loss: 623.3944 - val_mae: 19.9818\n",
      "Epoch 78/200\n",
      "824/824 [==============================] - 0s 76us/step - loss: 660.9049 - mae: 20.8141 - val_loss: 616.6833 - val_mae: 19.8566\n",
      "Epoch 79/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 653.8686 - mae: 20.6834 - val_loss: 610.0815 - val_mae: 19.7323\n",
      "Epoch 80/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 646.9889 - mae: 20.5539 - val_loss: 603.6594 - val_mae: 19.6106\n",
      "Epoch 81/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 640.1363 - mae: 20.4268 - val_loss: 597.1406 - val_mae: 19.4872\n",
      "Epoch 82/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 633.3660 - mae: 20.2976 - val_loss: 590.8401 - val_mae: 19.3673\n",
      "Epoch 83/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 626.6155 - mae: 20.1695 - val_loss: 584.5582 - val_mae: 19.2480\n",
      "Epoch 84/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 620.0131 - mae: 20.0424 - val_loss: 578.2877 - val_mae: 19.1316\n",
      "Epoch 85/200\n",
      "824/824 [==============================] - 0s 76us/step - loss: 613.4857 - mae: 19.9204 - val_loss: 572.1870 - val_mae: 19.0199\n",
      "Epoch 86/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 606.9306 - mae: 19.7949 - val_loss: 566.0859 - val_mae: 18.9070\n",
      "Epoch 87/200\n",
      "824/824 [==============================] - 0s 76us/step - loss: 600.5309 - mae: 19.6738 - val_loss: 560.1025 - val_mae: 18.7951\n",
      "Epoch 88/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 594.2310 - mae: 19.5542 - val_loss: 554.1989 - val_mae: 18.6842\n",
      "Epoch 89/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 587.9345 - mae: 19.4392 - val_loss: 548.3105 - val_mae: 18.5735\n",
      "Epoch 90/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 581.7534 - mae: 19.3211 - val_loss: 542.5344 - val_mae: 18.4637\n",
      "Epoch 91/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 575.5068 - mae: 19.1995 - val_loss: 536.6821 - val_mae: 18.3526\n",
      "Epoch 92/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 569.4481 - mae: 19.0856 - val_loss: 531.0451 - val_mae: 18.2447\n",
      "Epoch 93/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 563.4225 - mae: 18.9699 - val_loss: 525.4569 - val_mae: 18.1366\n",
      "Epoch 94/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 557.3750 - mae: 18.8542 - val_loss: 519.7111 - val_mae: 18.0250\n",
      "Epoch 95/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 551.3177 - mae: 18.7374 - val_loss: 514.1845 - val_mae: 17.9173\n",
      "Epoch 96/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 545.5320 - mae: 18.6231 - val_loss: 508.7850 - val_mae: 17.8118\n",
      "Epoch 97/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 539.6675 - mae: 18.5104 - val_loss: 503.3252 - val_mae: 17.7050\n",
      "Epoch 98/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 533.9722 - mae: 18.3960 - val_loss: 498.0968 - val_mae: 17.6029\n",
      "Epoch 99/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 528.3694 - mae: 18.2873 - val_loss: 492.8796 - val_mae: 17.4998\n",
      "Epoch 100/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 522.7603 - mae: 18.1764 - val_loss: 487.6802 - val_mae: 17.3965\n",
      "Epoch 101/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 517.2426 - mae: 18.0658 - val_loss: 482.5795 - val_mae: 17.2958\n",
      "Epoch 102/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 511.7638 - mae: 17.9538 - val_loss: 477.5132 - val_mae: 17.1954\n",
      "Epoch 103/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 506.4062 - mae: 17.8458 - val_loss: 472.5266 - val_mae: 17.0952\n",
      "Epoch 104/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 501.0547 - mae: 17.7379 - val_loss: 467.5722 - val_mae: 16.9945\n",
      "Epoch 105/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 495.7213 - mae: 17.6298 - val_loss: 462.6906 - val_mae: 16.8939\n",
      "Epoch 106/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 490.5052 - mae: 17.5279 - val_loss: 457.9120 - val_mae: 16.7944\n",
      "Epoch 107/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 485.4867 - mae: 17.4237 - val_loss: 453.3364 - val_mae: 16.7016\n",
      "Epoch 108/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 480.4209 - mae: 17.3215 - val_loss: 448.5914 - val_mae: 16.6068\n",
      "Epoch 109/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 475.3486 - mae: 17.2189 - val_loss: 443.9221 - val_mae: 16.5123\n",
      "Epoch 110/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 470.3944 - mae: 17.1203 - val_loss: 439.4726 - val_mae: 16.4224\n",
      "Epoch 111/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 465.6185 - mae: 17.0214 - val_loss: 434.9959 - val_mae: 16.3311\n",
      "Epoch 112/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 460.7354 - mae: 16.9222 - val_loss: 430.5771 - val_mae: 16.2414\n",
      "Epoch 113/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 456.0739 - mae: 16.8266 - val_loss: 426.3878 - val_mae: 16.1551\n",
      "Epoch 114/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 451.5147 - mae: 16.7327 - val_loss: 422.1737 - val_mae: 16.0683\n",
      "Epoch 115/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 446.8124 - mae: 16.6361 - val_loss: 417.8248 - val_mae: 15.9778\n",
      "Epoch 116/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "824/824 [==============================] - 0s 57us/step - loss: 442.2337 - mae: 16.5434 - val_loss: 413.7602 - val_mae: 15.8931\n",
      "Epoch 117/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 437.7797 - mae: 16.4505 - val_loss: 409.7006 - val_mae: 15.8078\n",
      "Epoch 118/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 433.3599 - mae: 16.3576 - val_loss: 405.6262 - val_mae: 15.7240\n",
      "Epoch 119/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 429.0889 - mae: 16.2692 - val_loss: 401.7883 - val_mae: 15.6476\n",
      "Epoch 120/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 424.7867 - mae: 16.1823 - val_loss: 397.8652 - val_mae: 15.5730\n",
      "Epoch 121/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 420.5898 - mae: 16.0950 - val_loss: 394.1233 - val_mae: 15.5031\n",
      "Epoch 122/200\n",
      "824/824 [==============================] - 0s 76us/step - loss: 416.3746 - mae: 16.0087 - val_loss: 390.2813 - val_mae: 15.4314\n",
      "Epoch 123/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 412.2788 - mae: 15.9287 - val_loss: 386.6522 - val_mae: 15.3642\n",
      "Epoch 124/200\n",
      "824/824 [==============================] - 0s 76us/step - loss: 408.3040 - mae: 15.8479 - val_loss: 382.9999 - val_mae: 15.2962\n",
      "Epoch 125/200\n",
      "824/824 [==============================] - 0s 76us/step - loss: 404.3213 - mae: 15.7674 - val_loss: 379.4731 - val_mae: 15.2306\n",
      "Epoch 126/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 400.5624 - mae: 15.6923 - val_loss: 376.0973 - val_mae: 15.1690\n",
      "Epoch 127/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 396.7582 - mae: 15.6167 - val_loss: 372.6847 - val_mae: 15.1065\n",
      "Epoch 128/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 392.9119 - mae: 15.5415 - val_loss: 369.2094 - val_mae: 15.0416\n",
      "Epoch 129/200\n",
      "824/824 [==============================] - 0s 76us/step - loss: 389.2006 - mae: 15.4702 - val_loss: 366.0224 - val_mae: 14.9824\n",
      "Epoch 130/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 385.7120 - mae: 15.4007 - val_loss: 362.9129 - val_mae: 14.9255\n",
      "Epoch 131/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 382.2129 - mae: 15.3350 - val_loss: 359.7608 - val_mae: 14.8667\n",
      "Epoch 132/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 378.7015 - mae: 15.2677 - val_loss: 356.6227 - val_mae: 14.8076\n",
      "Epoch 133/200\n",
      "824/824 [==============================] - ETA: 0s - loss: 558.3920 - mae: 18.63 - 0s 38us/step - loss: 375.1998 - mae: 15.1994 - val_loss: 353.5034 - val_mae: 14.7488\n",
      "Epoch 134/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 371.7946 - mae: 15.1375 - val_loss: 350.5855 - val_mae: 14.6942\n",
      "Epoch 135/200\n",
      "824/824 [==============================] - 0s 76us/step - loss: 368.4733 - mae: 15.0759 - val_loss: 347.5844 - val_mae: 14.6371\n",
      "Epoch 136/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 365.2387 - mae: 15.0128 - val_loss: 344.7908 - val_mae: 14.5827\n",
      "Epoch 137/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 362.0379 - mae: 14.9518 - val_loss: 342.0392 - val_mae: 14.5292\n",
      "Epoch 138/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 358.9343 - mae: 14.8935 - val_loss: 339.2720 - val_mae: 14.4764\n",
      "Epoch 139/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 355.8394 - mae: 14.8343 - val_loss: 336.6138 - val_mae: 14.4246\n",
      "Epoch 140/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 352.7877 - mae: 14.7755 - val_loss: 333.9018 - val_mae: 14.3731\n",
      "Epoch 141/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 349.9158 - mae: 14.7202 - val_loss: 331.5130 - val_mae: 14.3280\n",
      "Epoch 142/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 347.0670 - mae: 14.6637 - val_loss: 328.9971 - val_mae: 14.2805\n",
      "Epoch 143/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 344.3566 - mae: 14.6127 - val_loss: 326.7254 - val_mae: 14.2389\n",
      "Epoch 144/200\n",
      "824/824 [==============================] - 0s 76us/step - loss: 341.6207 - mae: 14.5591 - val_loss: 324.3233 - val_mae: 14.1956\n",
      "Epoch 145/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 338.9698 - mae: 14.5053 - val_loss: 322.1505 - val_mae: 14.1563\n",
      "Epoch 146/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 336.4995 - mae: 14.4579 - val_loss: 320.0591 - val_mae: 14.1174\n",
      "Epoch 147/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 334.0008 - mae: 14.4076 - val_loss: 317.9274 - val_mae: 14.0773\n",
      "Epoch 148/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 331.6296 - mae: 14.3612 - val_loss: 315.8830 - val_mae: 14.0389\n",
      "Epoch 149/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 329.3047 - mae: 14.3153 - val_loss: 313.9446 - val_mae: 14.0017\n",
      "Epoch 150/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 326.8631 - mae: 14.2644 - val_loss: 311.9224 - val_mae: 13.9643\n",
      "Epoch 151/200\n",
      "824/824 [==============================] - 0s 76us/step - loss: 324.6384 - mae: 14.2176 - val_loss: 310.1056 - val_mae: 13.9304\n",
      "Epoch 152/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 322.4177 - mae: 14.1731 - val_loss: 308.1940 - val_mae: 13.8936\n",
      "Epoch 153/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 320.2258 - mae: 14.1299 - val_loss: 306.4471 - val_mae: 13.8599\n",
      "Epoch 154/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 318.1791 - mae: 14.0898 - val_loss: 304.7361 - val_mae: 13.8290\n",
      "Epoch 155/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 316.1360 - mae: 14.0489 - val_loss: 303.0820 - val_mae: 13.7988\n",
      "Epoch 156/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 314.1899 - mae: 14.0090 - val_loss: 301.5137 - val_mae: 13.7693\n",
      "Epoch 157/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 312.2619 - mae: 13.9715 - val_loss: 299.9452 - val_mae: 13.7396\n",
      "Epoch 158/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 310.3799 - mae: 13.9333 - val_loss: 298.4342 - val_mae: 13.7111\n",
      "Epoch 159/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 308.5736 - mae: 13.8979 - val_loss: 297.0521 - val_mae: 13.6859\n",
      "Epoch 160/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 306.9066 - mae: 13.8652 - val_loss: 295.7539 - val_mae: 13.6622\n",
      "Epoch 161/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 305.3155 - mae: 13.8330 - val_loss: 294.5000 - val_mae: 13.6391\n",
      "Epoch 162/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 303.8097 - mae: 13.8023 - val_loss: 293.3644 - val_mae: 13.6194\n",
      "Epoch 163/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 302.4066 - mae: 13.7723 - val_loss: 292.2764 - val_mae: 13.6000\n",
      "Epoch 164/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 300.9051 - mae: 13.7406 - val_loss: 291.0855 - val_mae: 13.5808\n",
      "Epoch 165/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 299.4430 - mae: 13.7097 - val_loss: 290.0044 - val_mae: 13.5681\n",
      "Epoch 166/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 298.0842 - mae: 13.6802 - val_loss: 289.0384 - val_mae: 13.5569\n",
      "Epoch 167/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 296.8088 - mae: 13.6529 - val_loss: 288.0774 - val_mae: 13.5452\n",
      "Epoch 168/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 295.5588 - mae: 13.6292 - val_loss: 287.2198 - val_mae: 13.5345\n",
      "Epoch 169/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 294.4589 - mae: 13.6048 - val_loss: 286.4060 - val_mae: 13.5250\n",
      "Epoch 170/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 293.3335 - mae: 13.5829 - val_loss: 285.5954 - val_mae: 13.5150\n",
      "Epoch 171/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 292.2560 - mae: 13.5612 - val_loss: 284.8413 - val_mae: 13.5052\n",
      "Epoch 172/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 291.2074 - mae: 13.5418 - val_loss: 284.1399 - val_mae: 13.4956\n",
      "Epoch 173/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 290.2267 - mae: 13.5226 - val_loss: 283.5118 - val_mae: 13.4864\n",
      "Epoch 174/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 289.3747 - mae: 13.5059 - val_loss: 282.9240 - val_mae: 13.4793\n",
      "Epoch 175/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 288.4884 - mae: 13.4881 - val_loss: 282.3506 - val_mae: 13.4748\n",
      "Epoch 176/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 287.7060 - mae: 13.4736 - val_loss: 281.8897 - val_mae: 13.4718\n",
      "Epoch 177/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 286.9708 - mae: 13.4623 - val_loss: 281.4278 - val_mae: 13.4707\n",
      "Epoch 178/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 286.2779 - mae: 13.4508 - val_loss: 281.0138 - val_mae: 13.4707\n",
      "Epoch 179/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 285.6106 - mae: 13.4417 - val_loss: 280.6221 - val_mae: 13.4712\n",
      "Epoch 180/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 285.0109 - mae: 13.4340 - val_loss: 280.3053 - val_mae: 13.4722\n",
      "Epoch 181/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 284.4608 - mae: 13.4285 - val_loss: 280.0089 - val_mae: 13.4736\n",
      "Epoch 182/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 283.9357 - mae: 13.4216 - val_loss: 279.7256 - val_mae: 13.4777\n",
      "Epoch 183/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 283.4422 - mae: 13.4187 - val_loss: 279.4921 - val_mae: 13.4818\n",
      "Epoch 184/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 282.9941 - mae: 13.4158 - val_loss: 279.2845 - val_mae: 13.4858\n",
      "Epoch 185/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 282.6037 - mae: 13.4147 - val_loss: 279.1257 - val_mae: 13.4891\n",
      "Epoch 186/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 282.2396 - mae: 13.4130 - val_loss: 278.9598 - val_mae: 13.4931\n",
      "Epoch 187/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 281.8967 - mae: 13.4109 - val_loss: 278.8167 - val_mae: 13.4971\n",
      "Epoch 188/200\n",
      "824/824 [==============================] - 0s 76us/step - loss: 281.6014 - mae: 13.4095 - val_loss: 278.7232 - val_mae: 13.5007\n",
      "Epoch 189/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 281.3202 - mae: 13.4079 - val_loss: 278.6289 - val_mae: 13.5049\n",
      "Epoch 190/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 281.0805 - mae: 13.4088 - val_loss: 278.5657 - val_mae: 13.5085\n",
      "Epoch 191/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 280.8524 - mae: 13.4051 - val_loss: 278.4994 - val_mae: 13.5133\n",
      "Epoch 192/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 280.6115 - mae: 13.4062 - val_loss: 278.4496 - val_mae: 13.5182\n",
      "Epoch 193/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 280.4154 - mae: 13.4053 - val_loss: 278.4149 - val_mae: 13.5238\n",
      "Epoch 194/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 280.2214 - mae: 13.4047 - val_loss: 278.3883 - val_mae: 13.5301\n",
      "Epoch 195/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 280.0629 - mae: 13.4056 - val_loss: 278.3772 - val_mae: 13.5345\n",
      "Epoch 196/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 279.9256 - mae: 13.4047 - val_loss: 278.3727 - val_mae: 13.5399\n",
      "Epoch 197/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 279.7994 - mae: 13.4049 - val_loss: 278.3748 - val_mae: 13.5445\n",
      "Epoch 198/200\n",
      "824/824 [==============================] - 0s 57us/step - loss: 279.6860 - mae: 13.4042 - val_loss: 278.3846 - val_mae: 13.5500\n",
      "Epoch 199/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 279.5655 - mae: 13.4028 - val_loss: 278.4011 - val_mae: 13.5555\n",
      "Epoch 200/200\n",
      "824/824 [==============================] - 0s 38us/step - loss: 279.4805 - mae: 13.4033 - val_loss: 278.4184 - val_mae: 13.5599\n"
     ]
    }
   ],
   "source": [
    "model=built_model()\n",
    "history=model.fit(xtrain,ytrain,epochs=200,validation_data=(xtest,ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 2s 12ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44.217131086923544"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_mse_score, test_mae_score = model.evaluate(x_test_data,y_test_label)\n",
    "test_mse_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chatbot\n",
    "#dialougebox\n",
    "#guru \n",
    "#fiver\n",
    "#rest api\n",
    "#online parapharasing\n",
    "#refreasher \n",
    "# unique keyword "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
